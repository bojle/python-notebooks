{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ad64fd-c1c6-46f8-a299-0f2281564248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646bba41-f82b-4dac-a041-8184bc78a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifmap 2d, kernel 2d -> out 2d \n",
    "def _conv2d(ctx, ifmap, kernel):\n",
    "    \"\"\" conv2d helper - conv ifmap[i,j] with kernel \"\"\"\n",
    "    out = np.zeros(ctx.Hout * ctx.Wout)\n",
    "    out_index = 0\n",
    "    for i in range(ctx.Hout):\n",
    "        for j in range(ctx.Wout):\n",
    "            for ii in range(ctx.KH):\n",
    "                for jj in range(ctx.KW):\n",
    "                     #print(ifmap.shape)\n",
    "                     out[out_index] = out[out_index] + (ifmap[ii+i,jj+j] * kernel[ii, jj])                       \n",
    "            out_index = out_index + 1\n",
    "    return out.reshape(ctx.Hout, ctx.Wout)\n",
    "\n",
    "# ifmap 3d, kernel 3d -> out 2d (from addition of all the channels)\n",
    "def conv2d(ctx, ifmap, kernel):\n",
    "    out = np.empty((ctx.IC, ctx.Hout, ctx.Wout))\n",
    "    for i in range(ctx.IC):\n",
    "        out[i] = _conv2d(ctx, ifmap[0,i,:,:], kernel[i])\n",
    "    out = np.sum(out, axis=0)\n",
    "    return out\n",
    "\n",
    "class ctx:\n",
    "    def __init__(self, ifmap, kernels, stride=1, padding=0):\n",
    "        \"\"\" Args:\n",
    "                ifmap: input map\n",
    "                kernels: list of kernels each of size KH, KW\n",
    "            Returns:\n",
    "                ctx: object that collects N,C,IH,IW,KH,KW,Hout,Wout\n",
    "        \"\"\"\n",
    "        self.N, self.C, self.IH, self.IW = ifmap.shape\n",
    "        self.S, self.P = (stride, padding)\n",
    "        self.KN = kernels.shape[0]\n",
    "        self.IC = kernels.shape[1]\n",
    "        self.KH = kernels.shape[2]\n",
    "        self.KW = kernels.shape[3]\n",
    "        self.Hout = ceil((self.IW - self.KW)/stride) + 1\n",
    "        self.Wout = ceil((self.IH - self.KH)/stride) + 1\n",
    "\n",
    "def preprocess(image):\n",
    "    image = Image.open(image)\n",
    "    image = image.resize((224,224))\n",
    "    image = np.array(image).reshape((1,3,224,224))\n",
    "    # missing this \n",
    "    #image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def get_initializers(model):\n",
    "    model = onnx.load(model)\n",
    "    return model.graph.initializer\n",
    "    \n",
    "def get_kernel(model_name, layer_num):    \n",
    "    initializers = get_initializers(model_name)\n",
    "    p = re.compile('vgg0_conv{}_weight_quantized'.format(layer_num))\n",
    "    for i in initializers:\n",
    "        if p.match(i.name):\n",
    "            print(i.name)\n",
    "            array = np.frombuffer(i.raw_data, dtype=np.int8).reshape(i.dims)\n",
    "            return array\n",
    "\n",
    "def infer_layer(model, ifm, layer):\n",
    "    kernels = get_kernel(model, layer)\n",
    "    ctxo = ctx(ifm, kernels, stride=1, padding=0)\n",
    "    print(kernels.shape)\n",
    "    out = []\n",
    "    for i in range(kernels.shape[0]):\n",
    "        kernel = kernels[i]\n",
    "        out.append(conv2d(ctxo, ifm, kernel))\n",
    "    return np.array(out)\n",
    "\n",
    "def infer_layer_torch(model, ifm, layer):\n",
    "    input = torch.Tensor(ifm)\n",
    "    kernels = torch.Tensor(np.copy(get_kernel(model, layer))) \n",
    "    print(input)\n",
    "    print(kernels)\n",
    "    return torch.nn.functional.conv2d(input, kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab739201-c117-4f4a-9cd6-22cfa94cc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifm = preprocess(\"images/mug.jpg\")\n",
    "model_name = \"onnx/vgg/vgg16-12-int8.onnx\"\n",
    "#oo = infer_layer(model_name, ifm, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8844a3f0-2e99-4416-a348-b06eb1591fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28028f-03aa-4b7d-8268-4a80c5489d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = np.array(infer_layer_torch(model_name, ifm, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbe8c0-b5b8-4998-87f5-27c1a6397500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48be2cb-8c45-41fa-8238-f83d609867a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149, 148, 146, ..., 127, 129, 118],\n",
       "       [ 26,  16,   3, ...,  81, 112, 107],\n",
       "       [109, 130, 125, ..., 149, 151, 144],\n",
       "       ...,\n",
       "       [ 71,  67,  67, ..., 126, 119, 136],\n",
       "       [121, 113, 128, ...,  71,  63,  62],\n",
       "       [ 33,  31,  25, ...,  70,  67,  68]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ifm[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d11555c4-7129-4846-b709-323d31a67ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.arange(1,37).reshape(6,6).reshape(1,1,6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26512394-7357-4857-83a1-65153e30121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = np.arange(1,10).reshape(1,1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6007386-4a9f-4f14-bef3-5c4ef5f7ce55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 474.,  519.,  564.,  609.],\n",
       "          [ 744.,  789.,  834.,  879.],\n",
       "          [1014., 1059., 1104., 1149.],\n",
       "          [1284., 1329., 1374., 1419.]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.conv2d(torch.Tensor(inputs), torch.Tensor(kernels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d96e4d98-d112-49d5-94a4-cbed0a911aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0060a8e-5f5e-4196-815e-0da27db5ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
